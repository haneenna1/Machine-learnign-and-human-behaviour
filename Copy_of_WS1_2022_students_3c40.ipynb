{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wYFiyO1Kvf_F"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haneenna1/Machine-learnign-and-human-behaviour/blob/main/Copy_of_WS1_2022_students_3c40.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mLCUCd09-Y2"
      },
      "source": [
        "<center>\n",
        "<div>Machine Learning and Human Behavior - 236608 - Winter 2022-2023</div>\n",
        "<h1>Workshop #1 - Binary Choice ⚖️</h1>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions and submission guidelines\n",
        "\n",
        "* Clone this notebook and complete the exercise:\n",
        "    * Aim for clear and concise solutions.\n",
        "    * Indicate clearly with a text block the sections of your solutions.\n",
        "    * Answer dry questions in text (markdown) blocks, and wet questions in code blocks.\n",
        "* Submission guidelines:\n",
        "    * Add a text block in the beginning of your notebook with your IDs.\n",
        "    * When you're done, restart the notebook and make sure that everything runs smoothly (Runtime->\"Restart and Run All\")\n",
        "    * Export your notebook as ipynb (File->Download->\"Download .ipynb\")\n",
        "    * If you need to attach additional files to your submission (e.g images), add them to a zip file together with the notebook ipynb file.\n",
        "    * Submit through the course website. Remember to list partner IDs when you submit.\n",
        "* **Due date**: Monday 21/11/2022, 23:59\n",
        "* For any questions regarding this workshop task, contact [Eden](mailto:edens@campus.technion.ac.il).\n"
      ],
      "metadata": {
        "id": "yUMsz9ROZ85F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ8O1MKGeH0T"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "In the binary choice setting, users $u\\in U$ make a binary decision of whether or not to consume items $x \\in X$. Each item is represented by a vector $x\\in\\mathbb{R}^n$, and the outcome is represented using binary variable $y\\in\\left\\{0,1\\right\\}$, such that $y=1$ when the item was consumed. \n",
        "\n",
        "Given a predicate such as $x\\ge 0$, we denote its corresponding indicator function by $\\mathbb{1}(x\\ge 0)\\in\\{0,1\\}$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstract population models \n",
        "\n",
        "For the implementation of behavioral models, we define the abstract classes which handle data generation and formatting. These are similar to the abstract classes defined in HW1.\n",
        "\n",
        "As we will mostly use these classes through their public interface, there is no need to go through the implementation in detail."
      ],
      "metadata": {
        "id": "wYFiyO1Kvf_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import sklearn.metrics\n",
        "import sklearn.linear_model\n",
        "import sklearn.metrics\n",
        "\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "wczZ4yxtPxsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscreteChoiceEnvironment:\n",
        "    \"\"\"\n",
        "    Generic class for discrete-choice dataset generation\n",
        "    \"\"\"\n",
        "    n_features = 8\n",
        "    observations_per_user = 10\n",
        "    train_user_proportion = 0.6\n",
        "\n",
        "    def _generate_user_attributes(self, n_users):\n",
        "        \"\"\"\n",
        "        Generate latent parameters for users.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_users : int\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        users : ndarray of shape (n_users, n_features)\n",
        "        \"\"\"\n",
        "        return np.random.normal(\n",
        "            loc=1,\n",
        "            scale=0.1,\n",
        "            size=(\n",
        "                n_users,\n",
        "                self.n_features,\n",
        "            ),\n",
        "        )\n",
        "    \n",
        "    def _generate_item_attributes(self, n_users):\n",
        "        \"\"\"\n",
        "        Generate latent parameters for items.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_users : int\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        items : ndarray of shape\n",
        "                (n_users, observations_per_user, n_features)\n",
        "        \"\"\"\n",
        "        return np.random.normal(\n",
        "            size=(\n",
        "                n_users,\n",
        "                self.observations_per_user,\n",
        "                self.n_features,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def _choice(self, users, items):\n",
        "        \"\"\"\n",
        "        Discrete choice function\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        users : ndarray of shape (n_users, n_features)\n",
        "        items : ndarray of shape\n",
        "                (n_users, observations_per_user, n_features)\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        choice : Dict[str -> ndarray of shape(n_users, observations_per_user)]\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def _generate_choice_dataset(self, n_users):\n",
        "        \"\"\"\n",
        "        Generate choice dataset, formatted as pandas dataframe.\n",
        "        \"\"\"\n",
        "        users = self._generate_user_attributes(n_users)\n",
        "        items = self._generate_item_attributes(n_users)\n",
        "        choice_dct = self._choice(users, items)\n",
        "        rows = []\n",
        "        for i in range(n_users):\n",
        "            for j in range(self.observations_per_user):\n",
        "                dct = {}\n",
        "                dct['user_id'] = f'{i}'\n",
        "                for k in range(self.n_features):\n",
        "                        dct[f'x_{k}'] = items[i,j,k]\n",
        "                for choice_type, choice_matrix in choice_dct.items():\n",
        "                    dct[choice_type] = choice_matrix[i,j]\n",
        "                rows.append(dct)\n",
        "        df = pd.DataFrame(rows)\n",
        "        return df\n",
        "    \n",
        "    def generate_train_eval_datasets(self, n_users):\n",
        "        n_train_users = int(n_users*self.train_user_proportion)\n",
        "        n_test_users = n_users - n_train_users\n",
        "        return (\n",
        "            self._generate_choice_dataset(n_train_users),\n",
        "            self._generate_choice_dataset(n_test_users),\n",
        "        )\n",
        "\n",
        "    def get_feature_columns(self):\n",
        "        return [\n",
        "            f'x_{k}'\n",
        "            for k in range(self.n_features)\n",
        "        ]\n",
        "\n",
        "\n",
        "class InnerProductTrueValueEnvironment(DiscreteChoiceEnvironment):\n",
        "    @staticmethod\n",
        "    def _true_value(users, items):\n",
        "        # true_value is an inner product u@x.\n",
        "        # Calculate using np.einsum, where:\n",
        "        # * i: user index\n",
        "        # * j: observation (item) index\n",
        "        # * k: feature\n",
        "        true_value = np.einsum('ik,ijk->ij', users, items)\n",
        "        return true_value\n"
      ],
      "metadata": {
        "id": "j3tgZFC-y5pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task \\#1: Prediction with stated and revealed preferences\n"
      ],
      "metadata": {
        "id": "sZpLOZz3tzv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In our first task, we will investigate the relation between predictive performance, and the type of feedback obtained from users.\n",
        "\n",
        "We assume that consumption decisions are made according to the a random utility model. Each user is represented by a vector $u\\in\\mathbb{R}^d$, and each item is represented by a vector $x\\in\\mathbb{R}^d$. The true utility experienced by user $u$ from consuming item $x$ is assumed to be the inner product $v_u(x)=u^Tx$.\n",
        "\n",
        "We distinguish between three types of feedback:\n",
        "\n",
        "* **Rational preference**: When user $u$ is queried in an ideal environment, they consume the item if its utility is larger than zero. Their rational choice is to consume if the utility is larger than zero, hence $y_\\mathrm{rational} = \\mathbb{1}(v_u(x) \\ge 0)$.\n",
        "\n",
        "* **Stated preference**: When user $u$ is questioned explicitly about item $x$ (e.g in a survey), they tend to under-estimate the value of the item. Therefore, their stated consumption choice is given by $y_\\mathrm{stated} = \\mathbb{1}(v_u(x)-b \\ge 0)$, where $b\\ge 0$ is a fixed and latent bias term.\n",
        "\n",
        "* **Revealed preference**: When $u$ is presented with item item $x$, they reply according to a noisy evaluation $y_\\mathrm{revealed} = \\mathbb{1}(v_u(x)+\\varepsilon \\ge 0)$, where $\\varepsilon\\sim N(0,\\sigma)$.\n",
        "\n"
      ],
      "metadata": {
        "id": "KrG8O1lCDPXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1.1: Simulating user behavior\n",
        "\n",
        "The `NoisyBinaryChoiceEnvironment` class will be used for generating the datasets. It provides a simple interface which will be useful for simulation.\n",
        "\n"
      ],
      "metadata": {
        "id": "RYxGnvDZfF8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NoisyBinaryChoiceEnvironment(InnerProductTrueValueEnvironment):\n",
        "    \"\"\"\n",
        "    Dataset generator for binary choice with decision noise\n",
        "    \"\"\"\n",
        "    def __init__(self, noise_scale, bias=1):\n",
        "        self.noise_scale = noise_scale\n",
        "        self.bias = bias\n",
        "\n",
        "    def _choice(self, users, items):\n",
        "        true_value = self._true_value(users, items)\n",
        "        decision_noise = np.random.normal(\n",
        "            size=true_value.shape,\n",
        "            scale=self.noise_scale,\n",
        "        )\n",
        "        stated_value = true_value-self.bias\n",
        "        perceived_value = true_value + decision_noise\n",
        "        return {\n",
        "            'true_value': true_value,\n",
        "            'rational_choice': true_value >= 0,\n",
        "            'stated_choice': stated_value >= 0,\n",
        "            'revealed_choice': perceived_value >= 0,\n",
        "        }"
      ],
      "metadata": {
        "id": "xapLYncPJySz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "As an example, here we instantiate an environment with noise magnitude $\\sigma=2$, generate training and evaluation datasets with 1000 users. Note that the training and evaluation datasets are pandas DataFrames:"
      ],
      "metadata": {
        "id": "4tT5w6tiJxY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_noisy_choice_env = NoisyBinaryChoiceEnvironment(noise_scale=2)\n",
        "example_train_df, example_eval_df = example_noisy_choice_env.generate_train_eval_datasets(n_users=1000)\n",
        "example_train_df.head()"
      ],
      "metadata": {
        "id": "eWlkyaRCSglw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also extract features and train models using sklearn. Here we use the training set to fit a Logistic Regression model, and predict on the evaluation set:"
      ],
      "metadata": {
        "id": "jXq09m0moNyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_model = sklearn.linear_model.LogisticRegression().fit(\n",
        "    X=example_train_df[example_noisy_choice_env.get_feature_columns()],\n",
        "    y=example_train_df['revealed_choice'],\n",
        ")\n",
        "example_model.predict(\n",
        "    X=example_eval_df[example_noisy_choice_env.get_feature_columns()],\n",
        ")"
      ],
      "metadata": {
        "id": "a9vyetgzezYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Warm-up question**:\n",
        "\n",
        "In the `example_eval_df` dataset:\n",
        "* What is the proportion of positive consumption choices under the `rational_choice` criteria ($y_\\mathrm{rational}=1$)?\n",
        "* What is the proportion of positive consumption choices under the `stated_choice` criteria ($y_\\mathrm{stated}=1$)?\n",
        "* What is the proportion of positive consumption choices under the `revealed_choice` criteria ($y_\\mathrm{revealed}=1$)?\n",
        "\n",
        "🔵 **Answer**:"
      ],
      "metadata": {
        "id": "HmZA-a4bXNO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "n2QtE9S3XWrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1.2: Comparison graph\n",
        "\n",
        "Plot three line graphs (on the same figure) representing the accuracy of a linear regression model, for variable $\\sigma$. Plot one line graph for *stated preferences*, and another line for *revealed preferences*.  \n",
        "\n",
        "For each decision noise magnitude $\\sigma$ in the range $\\sigma\\in[0.1,20]$:\n",
        "* Instantiate a `NoisyBinaryChoiceEnvironment` environment with the given magnitude, and generate training/evaluation datasets with `n_users=20`. \n",
        "* Use the training set to train three Logistic Regression models, using $y_\\mathrm{rational}$, $y_\\mathrm{stated}$ and $y_\\mathrm{revealed}$ as training labels.\n",
        "* Evaluate model accuracy on predicting $y_\\mathrm{revealed}$ of the evaluation set.\n",
        "\n",
        "To reduce randomization noise, repeat the experiment 20 times for each $\\sigma$, and average the results.\n",
        "\n",
        "Hints:\n",
        "* Code should be simple and concise. Don't reinvent the wheel!\n",
        "* Given an environment `env` and a generated dataset `train_df`:\n",
        "  * `train_df['rational_choice']` is the user's rational choice $y_\\mathrm{rational}$. Similarly for $y_\\mathrm{stated}$, $y_\\mathrm{revealed}$.\n",
        "  * `train_df[env.get_feature_columns()]` extracts the feature columns from the DataFrame.\n",
        "* Use `sklearn.linear.LogisticRegression` as the prediction model:\n",
        "  * Note that pandas DataFrames are valid datatypes for sklearn's `X` and `y` arguments.\n",
        "  * Given a trained Logistic Regression model `m`, the command `m.score(X,y)` returns the mean accuracy on the given test data and labels.\n",
        "* Figures should be clear and organized. Make sure that title, axis labels, and legend are added and clearly labeled.\n",
        "\n",
        "🔵 **Answer**:"
      ],
      "metadata": {
        "id": "ss6i9HrgP6Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_scale_vec = np.linspace(0.1,20,10)\n",
        "task1_n_repetitions = 20\n",
        "task1_n_users = 20\n",
        "\n",
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "PclEQ6jtybAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain the results in detail. In your answer, relate to:\n",
        "* Overall trends: \n",
        "  * Are the lines increasing/decreasing/constant as a function of $\\sigma$? Why?\n",
        "* Relation between the lines: \n",
        "  * Assuming that $y_\\mathrm{rational}$ can't be measured in practice - Which of the other methods is better when $\\sigma\\to 0$? which method is better when $\\sigma\\to \\infty$? \n",
        "  * If lines cross each other, when and why do they cross? \n",
        "  * If lines coincide, when and why do they coincide?\n",
        "* Range of values: \n",
        "  * What is the range of accuracy values obtained? \n",
        "  * How do they relate to upper/lower bounds of predictive performance?\n"
      ],
      "metadata": {
        "id": "0AqNrFftoqdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "🔵 **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ],
      "metadata": {
        "id": "iTG-hUXoohG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1.3: Evaluating Welfare\n",
        "\n",
        "Plot a similar graph for the welfare metric you created in HW1:\n",
        "\n",
        "$$\n",
        "\\mathrm{welfare}(f, S)=\\frac{1}{|S|}\\sum_{ (u,x) \\in S } f(x) v_u(x)\n",
        "$$\n",
        "\n",
        "🔵 **Answer**:"
      ],
      "metadata": {
        "id": "DXoWHCAgOlcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "Xs9y8kbUOuFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain your results:\n",
        "\n",
        "🔵 **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ],
      "metadata": {
        "id": "duBbsnybOuFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1.4: Welfare counter-example\n",
        "\n",
        "**Assuming we use a linear classifer** (i.e. $y = \\mathrm{sign}(w^Tx + b)$)\n",
        "\n",
        "Find a dataset for which the classifier which yields the best accuracy, does not promote optimal welfare. Explain your answer.\n",
        "\n",
        "In your answer, you should provide:\n",
        "* Items and features\n",
        "* Utility function (utility doen't have to be linear)\n",
        "* Present a classifier, and explain why it's optimal in terms of accuracy.\n",
        "* Present another classifier which may have worse accuracy but better welfare.\n",
        "\n",
        "Note: you can sketch a 1D/2D dataset using Power-Point (taking a screenshot), or through this website: https://app.diagrams.net/. Describe the solution in words below. \n",
        "\n",
        "Attach the diagram inside a zip file together with your notebook when you submit your solution.\n",
        "\n",
        "🔵 **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ],
      "metadata": {
        "id": "Oq91WJDZBqTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1.5: Exploratory Analysis (Open-Ended)"
      ],
      "metadata": {
        "id": "2dAOQwmwOxdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Looking at both measures (accuracy and welfare), try to vary the parameters of the experiment (\\#users, \\#items, \\#features, etc.) in ways that show interesting trends. Explain your results in detail and support your claims.\n",
        "\n",
        "🔵 **Answer**:"
      ],
      "metadata": {
        "id": "V5CvkB8RBTTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "Xu61KphuCM1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain your results:\n",
        "\n",
        "🔵 **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ],
      "metadata": {
        "id": "oJY_vDLJCMmm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD7iSr_MlTPd"
      },
      "source": [
        "# Task \\#2: Rationality assumptions in loss-averse environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa80uj8SmX4b"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this task, we will evaluate the performance of a standard (\"rational\") prediction model when decision-makers are loss-averse. \n",
        "\n",
        "In this section, users make decisions under uncertainty. Each user has two inherent utility functions, $u_a$ and $u_b$, and a probability parameter $p\\in\\left[0,1\\right]$. The user's utility from consuming an item $x$ is:\n",
        "- $u_a(x)\\in\\mathbb{R}$ with probability $p$, and \n",
        "- $u_b(x)\\in\\mathbb{R}$ with probability $(1-p)$. \n",
        "\n",
        "When user behavior is *rational*, the decision is made by comparing the *expected utility* of the two alternatives:\n",
        "\n",
        "$$\n",
        "y_\\text{rational}=\\begin{cases}\n",
        "1&p \\cdot u_a(x) + (1-p) \\cdot u_b(x) \\ge 0\\\\\n",
        "0&\\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "When user decisions are subject to *behavioral bias*, choice deviates from the expected optiumum. In particular, we will focus on a setting where the users are *loss-averse*. In the spirit of Prospect Theory [[1](https://en.wikipedia.org/wiki/Prospect_theory)], we assume there exist two functions $\\pi, v$ such that the perceived value from consuming the item is:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "V_{\\pi, v}\\left(x\\right)\n",
        "&=\\sum_{i=1}^2 \\pi\\left(p_i\\right) v\\left(u_i(x)\\right)\\\\\n",
        "&=\\pi(p) \\cdot v(u_a(x)) + \\pi(1-p) \\cdot v(u_b(x))\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The function $v$ captures the loss-aversion property, and it is s-shaped and asymmetrical. The function $\\pi$ is a probability weighting function and captures the idea that people tend to overreact to small probability events, but underreact to large probabilities. Assuming $v(0)=0$, consumption decisions are made according to the following rule:\n",
        "\n",
        "$$\n",
        "y_\\text{prospect}=\\begin{cases}\n",
        "1&V_{\\pi,v}(x) \\ge 0\\\\\n",
        "0&\\text{otherwise}\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnXadiQab3hl"
      },
      "source": [
        "## Exercise 2.1: Understanding the functional form of $v$\n",
        "\n",
        "\n",
        "The user valuation bias can be modeled using an S-shaped assymetrical function $v:\\mathbb{R}\\to\\mathbb{R}$. Following [[2](https://www.econstor.eu/bitstream/10419/87132/1/472515071.pdf)], we assume that $v$ is a power S-shaped utility function, and its functional form is given by:\n",
        "\n",
        "\n",
        "$$\n",
        "v(u)=\\begin{cases}\n",
        "u^\\alpha& u \\ge 0 \\\\\n",
        "-\\gamma \\left(-u\\right)^\\beta& u < 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "where $u=u(x)$ is the objective utility from consuming item $x$, and $0< \\alpha \\le \\beta \\le 1$, $\\gamma\\ge 1$ are constants.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation**\n",
        "\n",
        "Implement the class `PowerLossAversion`. The class constructor will receive three scalar constants `alpha`, `beta`, `gamma`.  The `__call__` function will calculate $v(u)$ as defined above.\n",
        "\n",
        "Hint: Make your code more efficient by using numpy vectorized operations, and avoid explicit loops and if statements.\n",
        "\n",
        "🔵 **Answer**:"
      ],
      "metadata": {
        "id": "ik5BKrVECxQg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4cfUhgvG-dY"
      },
      "source": [
        "class PowerLossAversion:\n",
        "    \"\"\"\n",
        "    The power S-shaped utility function, as defined by Maggi (2014)\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha, beta, gamma):\n",
        "        assert 0 < alpha <= 1\n",
        "        assert 0 < beta <= 1\n",
        "        assert alpha <= beta\n",
        "        assert gamma >= 1\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "    \n",
        "    def __call__(self, u):\n",
        "        \"\"\"\n",
        "        Compute the power S-shaped utility function for a vector of utilities.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        u : ndarray of shape (n)\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        v : ndarray of shape (n)\n",
        "        \"\"\"\n",
        "        ## YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8D62Fpxm9zk"
      },
      "source": [
        "Using the implementation above, plot the function $\\nu$ for values of $u$ in the range $[-2,2]$, and for the given sets of parameters:\n",
        "\n",
        "  1. $\\left(\\alpha_1, \\beta_1, \\gamma_1\\right) = \\left(1, 1, 1\\right)$\n",
        "  2. $\\left(\\alpha_2, \\beta_2, \\gamma_2\\right) = \\left(1, 1, 2.5\\right)$\n",
        "  3. $\\left(\\alpha_3, \\beta_3, \\gamma_3\\right) = \\left(0.88, 0.88, 2.5\\right)$\n",
        "  4. $\\left(\\alpha_4, \\beta_4, \\gamma_4\\right) = \\left(0.2, 0.88, 1.8\\right)$\n",
        "\n",
        "🔵 **Answer**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oChQJkNmzee"
      },
      "source": [
        "prospect_params_lst = [\n",
        "    (1,1,1),\n",
        "    (1,1,2.5),\n",
        "    (0.88,0.88,2.5),\n",
        "    (0.2,0.8,1.8),\n",
        "]\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(10,4))\n",
        "\n",
        "u_vec = np.linspace(-1.5,1.5,200)\n",
        "for alpha, beta, gamma in prospect_params_lst:\n",
        "    v = PowerLossAversion(alpha, beta, gamma)\n",
        "    perceived_value = v(u_vec)\n",
        "    ax.plot(\n",
        "        u_vec,\n",
        "        perceived_value,\n",
        "        label=f'$\\\\alpha={alpha}, \\\\beta={beta}, \\\\gamma={gamma}$',\n",
        "    )\n",
        "\n",
        "ax.set_title('Illustration of Loss Aversion Functions $v(u; \\\\alpha, \\\\beta, \\\\gamma)$')\n",
        "ax.set_xlabel('objective utility ($u$)')\n",
        "ax.set_ylabel(r'perceived value ($v(u)$)')\n",
        "ax.axhline(0,linestyle=':',zorder=-1,alpha=0.3)\n",
        "ax.axvline(0,linestyle=':',zorder=-1,alpha=0.3)\n",
        "ax.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4B_uSzKqi17"
      },
      "source": [
        "Given the results - \n",
        "\n",
        "What type of behavior is characterized by the curve parametrized by $\\left(\\alpha, \\beta, \\gamma\\right) = \\left(1, 1, 1\\right)$?\n",
        "\n",
        "🔵 **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "What is the interpretation of the parameter $\\gamma$? Which behavioral traits are represented by high/low values of $\\gamma$? What aspect of prospect theory does it correspond to?\n",
        "\n",
        "🔵 **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)\n",
        "\n"
      ],
      "metadata": {
        "id": "MEbT-TFJQbyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the meaning of the parameters $\\alpha,\\beta$? What aspects of prospect theory do they correspond to?\n",
        "\n",
        "🔵 **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)\n",
        "\n"
      ],
      "metadata": {
        "id": "jEw77v4TQfHI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzcoyoM6PeGF"
      },
      "source": [
        "## Exercise 2.2: Simulating user behavior"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example**"
      ],
      "metadata": {
        "id": "FtRd0z0qfF9X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWzpj8cLJNdu"
      },
      "source": [
        "For the implementation of this behavioral model, we inherit from the `InnerProductTrueValueEnvironment` defined at the start of this notebook, and define the following abstract class:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ProspectEnvironment(InnerProductTrueValueEnvironment):\n",
        "    def _generate_user_attributes(self, n_users):\n",
        "        \"\"\"\n",
        "        Generate latent parameters for users.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_users : int\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        users : ndarray of shape (n_outcomes, n_users, n_features)\n",
        "        \"\"\"\n",
        "        return np.stack(\n",
        "            [\n",
        "                np.random.normal(\n",
        "                    loc=1,\n",
        "                    scale=0.1,\n",
        "                    size=(n_users, self.n_features),\n",
        "                ),\n",
        "                np.random.normal(\n",
        "                    loc=-0.1,\n",
        "                    scale=0.1,\n",
        "                    size=(n_users, self.n_features),\n",
        "                ),\n",
        "            ],\n",
        "            axis=0,\n",
        "        )"
      ],
      "metadata": {
        "id": "1cOwNd4cc-hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can inherit from these classes to create specific behavioral models. For example, here is a class which models unbiased decision making:"
      ],
      "metadata": {
        "id": "-q0IqcuYc-Fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RationalProspectEnvironmentExample(ProspectEnvironment):\n",
        "    def __init__(self):\n",
        "        p_a = np.random.uniform(0,1)\n",
        "        self.p = [p_a, 1-p_a]\n",
        "        super().__init__()\n",
        "\n",
        "    def _choice(self, users, items):\n",
        "        \"\"\"\n",
        "        Simulate choice\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        users : ndarray of shape (n_outcomes, n_users, n_features)\n",
        "        items : ndarray of shape (n_users, n_observations, n_features)\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        choice : Dict[str -> ndarray of shape(n_users, observations_per_user)]\n",
        "        \"\"\"\n",
        "        # Calculate true value based on inner product\n",
        "        u_a = self._true_value(users[0], items)\n",
        "        u_b = self._true_value(users[1], items)\n",
        "        return {\n",
        "            'u_a': u_a,  # u_a(x)\n",
        "            'u_b': u_b,  # u_b(x)\n",
        "            'rational_choice': self.p[0]*u_a + self.p[1]*u_b >= 0,\n",
        "        }\n",
        "\n",
        "rational_env_example = RationalProspectEnvironmentExample()\n",
        "rational_train_df, rational_eval_df = rational_env_example.generate_train_eval_datasets(n_users=100)\n",
        "rational_train_df.sample(5)"
      ],
      "metadata": {
        "id": "1C3nxIBHczmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXadaa3mPZkP"
      },
      "source": [
        "**Implementation**\n",
        "\n",
        "Based on the example above, implement the `BehavioralProspectEnvironment` class for simulating choice in a behavioral environment:\n",
        "* Class should inherit from `ProspectEnvironment`\n",
        "* Prospect value function $v(u)$ and a probability weighting function $\\pi(p)$ should be given in the class constructor.\n",
        "* Generate the probability $p$ uniformly in $[0, 1]$.\n",
        "* Implement the binary choice inside the `_choice` function. Function returns a dictionary mapping column names to numpy arrays containing their contents (see example above).\n",
        "\n",
        "🔵 **Answer**:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "ab_eRt7jD58T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxa6aSrGshTc"
      },
      "source": [
        "## Exercise 2.3: Predicting under behavioral bias\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhNb9u_Yvt8O"
      },
      "source": [
        "For each set of behavioral parameters $\\left(\\alpha_1, \\beta_1, \\gamma_1\\right),\\dots,\\left(\\alpha_4, \\beta_4, \\gamma_4\\right)$ given above, and for a neutral probability weighting ($\\pi(p)=p$), train and evaluate a Logistic Regression model on data generated by the corresponding `BehavioralProspectEnvironment`, with `n_users=100`.\n",
        "\n",
        "Report evaluation set accuracy for each set of parameters, averaged over 10 repetitions of the simulation.\n",
        "\n",
        "🔵 **Answer**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoNMaks5x-Ro"
      },
      "source": [
        "task_2_3_n_repetitions = 10\n",
        "task_2_3_n_users = 100\n",
        "\n",
        "## YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yi6vf5ky5aW"
      },
      "source": [
        "Plot a line graph representing the accuracy, for fixed $\\alpha=\\beta=1$ and variable $\\gamma\\in[1,15]$. Repeat each simulation 10 times for each value of $\\gamma$, and use the average value for the plot.\n",
        "\n",
        "🔵 **Answer**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCLjej7uxdLO"
      },
      "source": [
        "alpha = 1\n",
        "beta = 1\n",
        "gamma_vec = np.linspace(1,5,10)\n",
        "\n",
        "## YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehCgoCVD1J00"
      },
      "source": [
        "Explain the results: \n",
        "\n",
        "🔵 **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc5nHH982FTR"
      },
      "source": [
        "Similarly, plot two lines representing the accuracy, for fixed $\\gamma=\\{1,2\\}$ and variable $\\alpha=\\beta\\in[0.5,1]$. Repeat each simulation 10 times for each value of $\\gamma$, and average results.\n",
        "\n",
        "🔵 **Answer**:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alphabeta_vec = np.linspace(0.4,1,20)\n",
        "gamma=[1,2]\n",
        "\n",
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "fXdXRxnRBJNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain your results: \n",
        "\n",
        "🔵 **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ],
      "metadata": {
        "id": "A4HdE0EINOMm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGBjVdMB6A23"
      },
      "source": [
        "\n",
        "What can we conclude about the performance of a logistic regression classifier on behavioral data? What can explain the above observations?\n",
        "\n",
        "🔵 **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYtTZyoz31qF"
      },
      "source": [
        "## Exercise 2.4: Exploratory Analysis (Open-Ended)\n",
        "\n",
        "We present three open-ended questions. Please choose one of them and answer in detail. **Bonus will be given for solving more than one question**.\n",
        "\n",
        "This task is exploratory, and we encourage you to try different and creative approaches to solve it. In your answers, you should design and run appropriate experiment(s) - state your hypotheses, show plots that support your claim, and explain them.\n",
        "\n",
        "### Option 1: Alternative probability weighting\n",
        "\n",
        "Up until now, the decision model we analyzed assumed $\\pi$ to be neutral ($\\pi(p)=p$). A model with the following $\\pi$ is proposed:\n",
        "\n",
        "$$\\pi(p)=(1-p)\\cdot\\sqrt{p}+p\\cdot(1-\\sqrt{1-p})\n",
        "$$\n",
        "\n",
        "Will this change the accuracy analysis results? If so, how and in which direction? If not, why? Explain, and run experiments to support your claims.\n",
        "\n",
        "### Option 2: Estimating behavioral deviations\n",
        "\n",
        "Assuming neutral $\\pi$ ($\\pi(p)=p$) and a power S-shaped utility function $v$ (as described in Ex. 2.1), propose a way to estimate the functional parameters $\\alpha,\\beta,\\gamma$ from data. Support your claims using simulated data.\n",
        "\n",
        "### Option 3: Accounting for behavioral deviations\n",
        "\n",
        "Assuming neutral $\\pi$ ($\\pi(p)=p$) and a power S-shaped utility function $v$ (as described in Ex. 2.1), significantly improve predictive performance compared to the naive logistic regression baseline. \n",
        "\n",
        "Support your claims using simulated data, and evaluate performance on behavioral models with parameters $\\alpha, \\beta, \\gamma$ as defined in `prospect_params_lst` above.\n",
        "Explain your methods. How did you train your model? Why?\n",
        "\n",
        "\n",
        "🔵 **Answer**:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "WTYNP6SLNkTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain your results: \n",
        "\n",
        "🔵 **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ],
      "metadata": {
        "id": "nYN2j6uiVsRe"
      }
    }
  ]
}